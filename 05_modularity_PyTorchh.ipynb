{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modularity in PyTorch"
      ],
      "metadata": {
        "id": "yeF4-b0mOGW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clone repository\n",
        "!git clone https://github.com/andrii4k-kit/pytorch-learning\n",
        "%cd pytorch-learning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVGY4YOMUVlx",
        "outputId": "710fc02b-1a6b-4f02-8e56-338a3db18b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-learning'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 46 (delta 20), reused 10 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (46/46), 4.59 MiB | 11.75 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "/content/pytorch-learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 2. Create a folder for scripts\n",
        "project_path = \"scripts\"\n",
        "if not os.path.exists(project_path):\n",
        "    os.makedirs(project_path, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "CI3l5c2tU2lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pytorch-learning/05_scripts_modularity/scripts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NNKKbu0Z06D",
        "outputId": "c4deac75-dafa-4cb5-94ff-4fa84dc459dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-learning/05_scripts_modularity/scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUycmo9M3zt0",
        "outputId": "adddf1fd-c8a4-4f1e-c603-73ce9668f931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Datasets and Dataloaders & save as a script `(data_set_load.py)`"
      ],
      "metadata": {
        "id": "bTWVM2CUXxD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/data_set_load.py\n",
        "\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "  \"\"\"\n",
        "    Converts image directories into training and testing DataLoaders.\n",
        "\n",
        "    Handles the full dataset creation process including directory mapping,\n",
        "    preprocessing via transforms, and batching. Simplifies the data pipeline\n",
        "    setup for image classification models.\n",
        "\n",
        "    Args:\n",
        "        train_dir (str): Directory for training samples.\n",
        "        test_dir (str): Directory for testing samples.\n",
        "        transform (transforms.Compose): Preprocessing steps to apply.\n",
        "        batch_size (int): Samples per batch.\n",
        "        num_workers (int): Workers for parallel loading (default is CPU count).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_dataloader, test_dataloader, class_names)\n",
        "  \"\"\"\n",
        "  # Use ImageFolder to create dataset\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False, # don't need to shuffle test data\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kXrcNEGZqcu",
        "outputId": "d5b9ab91-ae9c-48cd-f6d9-5c08c64093d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/data_set_load.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a model & save as a script `(model_builder.py)`"
      ],
      "metadata": {
        "id": "GSSQsH10cCua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/model_builder.py\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"\n",
        "    Implementation of the TinyVGG convolutional neural network architecture.\n",
        "\n",
        "    Args:\n",
        "        input_shape (int): Number of input channels (e.g., 3 for RGB images).\n",
        "        hidden_units (int): Number of neurons/filters in the hidden layers.\n",
        "        output_shape (int): Number of output classes for the final prediction.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "      super().__init__()\n",
        "      self.conv_block_1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=input_shape,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2,\n",
        "                        stride=2)\n",
        "      )\n",
        "      self.conv_block_2 = nn.Sequential(\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "      )\n",
        "      self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          # Where did this in_features shape come from?\n",
        "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "          nn.Linear(in_features=hidden_units*13*13,\n",
        "                    out_features=output_shape)\n",
        "      )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "      x = self.conv_block_1(x)\n",
        "      x = self.conv_block_2(x)\n",
        "      x = self.classifier(x)\n",
        "      return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF2CzVywcWI7",
        "outputId": "9c14365d-d4ec-4923-806f-1c6e840eeb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Train_step, Test_step, and train scripts `(train_step.py), (test_step.py), (train.py)`"
      ],
      "metadata": {
        "id": "zJzIgiEvdD5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"\n",
        "    Performs one training iteration over the provided DataLoader.\n",
        "\n",
        "    Integrates the forward pass, backpropagation, and weight updates within\n",
        "    a single epoch.\n",
        "\n",
        "    Args:\n",
        "        model, dataloader, loss_fn, optimizer, device: Standard PyTorch training components.\n",
        "\n",
        "    Returns:\n",
        "        Metrics for training loss and accuracy as a tuple.\n",
        "  \"\"\"\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "      # Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # 2. Calculate  and accumulate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backward\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate and accumulate accuracy metric across all batches\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"\n",
        "    Performs a single evaluation epoch over the provided DataLoader.\n",
        "\n",
        "    Computes average loss and accuracy metrics across the test set using\n",
        "    inference mode.\n",
        "\n",
        "    Args:\n",
        "        model, dataloader, loss_fn, device: Standard PyTorch evaluation components.\n",
        "\n",
        "    Returns:\n",
        "        Metrics for test loss and accuracy as a tuple.\n",
        "  \"\"\"\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "      # Loop through DataLoader batches\n",
        "      for batch, (X, y) in enumerate(dataloader):\n",
        "          # Send data to target device\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # 1. Forward pass\n",
        "          test_pred_logits = model(X)\n",
        "\n",
        "          # 2. Calculate and accumulate loss\n",
        "          loss = loss_fn(test_pred_logits, y)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # Calculate and accumulate accuracy\n",
        "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "  \"\"\"\n",
        "    Main training loop for model optimization and testing.\n",
        "\n",
        "    Iterates through training and testing cycles for a specified number of epochs,\n",
        "    logging average batch metrics at each stage to monitor convergence.\n",
        "\n",
        "    Args:\n",
        "        model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device:\n",
        "        Core components and hyperparameters for the training process.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List]: Historical performance data for loss and accuracy across epochs.\n",
        "  \"\"\"\n",
        "  # Create empty results dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "      train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "      test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "      # Print out what's happening\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "      )\n",
        "\n",
        "      # Update results dictionary\n",
        "      results[\"train_loss\"].append(train_loss)\n",
        "      results[\"train_acc\"].append(train_acc)\n",
        "      results[\"test_loss\"].append(test_loss)\n",
        "      results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3Grquhkdd5n",
        "outputId": "d6b455af-70a9-4444-faba-4985072e7ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a function for saving the model `(utils.py)`"
      ],
      "metadata": {
        "id": "bcaedvIfeu4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/utils.py\n",
        "\"\"\"\n",
        "Contains various utility functions for PyTorch model training and saving.\n",
        "\"\"\"\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "  \"\"\"\n",
        "    Saves the trained model weights to disk.\n",
        "\n",
        "    Handles directory creation and validates the file extension before saving\n",
        "    the model's state_dict.\n",
        "\n",
        "    Args:\n",
        "        model, target_dir, model_name: Model instance, output path, and filename.\n",
        "  \"\"\"\n",
        "  # Create target directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "  # Create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # Save the model state_dict()\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhTyYn-se9rf",
        "outputId": "425ab10f-6b3e-4876-d282-0f6a6ec61282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, evaluate and save the model `(train.py)`"
      ],
      "metadata": {
        "id": "KeOcLQ8Gf7cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/train.py\n",
        "\"\"\"\n",
        "Trains a PyTorch image classification model using device-agnostic code.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import data_set_load, engine, model_builder, utils\n",
        "from torchvision import transforms\n",
        "\n",
        "import argparse\n",
        "# Initialize ArgumentParser\n",
        "parser = argparse.ArgumentParser(description=\"Get some hyperparameters.\")\n",
        "\n",
        "parser.add_argument(\"--train_dir\", type=str, help=\"Directory for training data.\", default=\"data/pizza_steak_sushi/train\")\n",
        "parser.add_argument(\"--test_dir\", type=str, help=\"Directory for test data.\", default=\"data/pizza_steak_sushi/test\")\n",
        "parser.add_argument(\"--NUM_EPOCHS\", type=int, help=\"The amount of epochs.\", default=5)\n",
        "parser.add_argument(\"--BATCH_SIZE\", type=int, help=\"Amount of img in batch.\", default=32)\n",
        "parser.add_argument(\"--HIDDEN_UNITS\", type=int, help=\"Amount of parameters/neurons per layer.\", default=10)\n",
        "parser.add_argument(\"--LEARNING_RATE\", type=float, help=\"Learning Rate for the optimizer.\", default=0.001)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "# Setup hyperparameters\n",
        "NUM_EPOCHS = args.NUM_EPOCHS\n",
        "BATCH_SIZE = args.BATCH_SIZE\n",
        "HIDDEN_UNITS = args.HIDDEN_UNITS\n",
        "LEARNING_RATE = args.LEARNING_RATE\n",
        "\n",
        "# Setup directories\n",
        "train_dir = args.train_dir\n",
        "test_dir = args.test_dir\n",
        "\n",
        "\n",
        "\n",
        "# Setup target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create transforms\n",
        "data_transform = transforms.Compose([\n",
        "  transforms.Resize((64, 64)),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create DataLoaders with help from data_setup.py\n",
        "train_dataloader, test_dataloader, class_names = data_set_load.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Create model with help from model_builder.py\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=HIDDEN_UNITS,\n",
        "    output_shape=len(class_names)\n",
        ").to(device)\n",
        "\n",
        "# Set loss and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "# Start training with help from engine.py\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "# Save the model with help from utils.py\n",
        "utils.save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHlR3RDafaVN",
        "outputId": "28c804a3-8954-4434-a8c8-1bd657706c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"hggd.andr@gmail.com\"\n",
        "!git config --global user.name \"andrii4k-kit\""
      ],
      "metadata": {
        "id": "fOAEinU6gg7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add 05_scripts_modularity/\n",
        "!git commit -m \"Add modular PyTorch scripts for chapter 05\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVWoIOnGgrrh",
        "outputId": "370499d4-87fb-43f3-b06f-de62456655ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main b07afa9] Add modular PyTorch scripts for chapter 05\n",
            " 7 files changed, 23 insertions(+), 7 deletions(-)\n",
            " create mode 100644 05_scripts_modularity/scripts/__pycache__/data_set_load.cpython-312.pyc\n",
            " create mode 100644 05_scripts_modularity/scripts/__pycache__/engine.cpython-312.pyc\n",
            " create mode 100644 05_scripts_modularity/scripts/__pycache__/model_builder.cpython-312.pyc\n",
            " create mode 100644 05_scripts_modularity/scripts/__pycache__/utils.cpython-312.pyc\n",
            " create mode 100644 05_scripts_modularity/scripts/models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHB7fasGhl27",
        "outputId": "568802b4-6d85-4bb2-b8a7-888fad3f112d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -d */"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQh_AXXvg9Mr",
        "outputId": "04ed6db8-195c-4921-9c20-3e6484be5997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scripts/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 05_scripts_modularity/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTSnuWHlg_nA",
        "outputId": "19cc9f2d-a4c4-4217-9c44-ac5a8e850439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '05_scripts_modularity/'\n",
            "/content/pytorch-learning/05_scripts_modularity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd -"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoUZNQdZhLgf",
        "outputId": "02457ff8-5cd5-4eeb-c651-730f8cd4d1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push https://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@github.com/andrii4k-kit/pytorch-learning.git main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCV-QZNZiQUN",
        "outputId": "6552fbe5-0a7f-4bbd-9f24-ef8c4455a597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Permission to andrii4k-kit/pytorch-learning.git denied to andrii4k-kit.\n",
            "fatal: unable to access 'https://github.com/andrii4k-kit/pytorch-learning.git/': The requested URL returned error: 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Everything in one line code"
      ],
      "metadata": {
        "id": "DIAI4LSQH-sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --train_dir /content/data/pizza_steak_sushi/train --test_dir /content/data/pizza_steak_sushi/test --NUM_EPOCHS 50 --BATCH_SIZE 32 --HIDDEN_UNITS 10 --LEARNING_RATE 0.001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJzKPw9P2PH_",
        "outputId": "360d28b3-f215-447d-aef7-6c34e832fdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/50 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.1020 | train_acc: 0.3984 | test_loss: 1.1145 | test_acc: 0.1979\n",
            "  2% 1/50 [00:01<00:56,  1.15s/it]Epoch: 2 | train_loss: 1.0913 | train_acc: 0.4141 | test_loss: 1.1305 | test_acc: 0.1979\n",
            "  4% 2/50 [00:02<00:48,  1.02s/it]Epoch: 3 | train_loss: 1.0804 | train_acc: 0.4141 | test_loss: 1.1409 | test_acc: 0.1979\n",
            "  6% 3/50 [00:02<00:44,  1.05it/s]Epoch: 4 | train_loss: 1.1005 | train_acc: 0.2930 | test_loss: 1.1079 | test_acc: 0.1979\n",
            "  8% 4/50 [00:03<00:42,  1.08it/s]Epoch: 5 | train_loss: 1.0540 | train_acc: 0.5312 | test_loss: 1.0580 | test_acc: 0.2917\n",
            " 10% 5/50 [00:04<00:40,  1.10it/s]Epoch: 6 | train_loss: 1.0218 | train_acc: 0.5156 | test_loss: 1.1394 | test_acc: 0.3542\n",
            " 12% 6/50 [00:05<00:39,  1.11it/s]Epoch: 7 | train_loss: 0.9915 | train_acc: 0.5156 | test_loss: 0.9346 | test_acc: 0.5634\n",
            " 14% 7/50 [00:06<00:44,  1.04s/it]Epoch: 8 | train_loss: 1.0133 | train_acc: 0.4141 | test_loss: 1.0578 | test_acc: 0.3636\n",
            " 16% 8/50 [00:08<00:48,  1.16s/it]Epoch: 9 | train_loss: 0.9524 | train_acc: 0.5625 | test_loss: 1.2079 | test_acc: 0.3542\n",
            " 18% 9/50 [00:09<00:43,  1.07s/it]Epoch: 10 | train_loss: 0.9584 | train_acc: 0.4766 | test_loss: 1.0642 | test_acc: 0.3229\n",
            " 20% 10/50 [00:10<00:40,  1.01s/it]Epoch: 11 | train_loss: 0.8198 | train_acc: 0.6836 | test_loss: 0.9522 | test_acc: 0.5947\n",
            " 22% 11/50 [00:11<00:39,  1.01s/it]Epoch: 12 | train_loss: 0.8416 | train_acc: 0.5977 | test_loss: 1.0552 | test_acc: 0.3835\n",
            " 24% 12/50 [00:11<00:36,  1.04it/s]Epoch: 13 | train_loss: 1.0459 | train_acc: 0.5195 | test_loss: 1.0729 | test_acc: 0.4138\n",
            " 26% 13/50 [00:12<00:34,  1.07it/s]Epoch: 14 | train_loss: 0.8143 | train_acc: 0.6875 | test_loss: 1.0077 | test_acc: 0.4233\n",
            " 28% 14/50 [00:13<00:32,  1.10it/s]Epoch: 15 | train_loss: 0.8376 | train_acc: 0.5703 | test_loss: 1.0443 | test_acc: 0.3835\n",
            " 30% 15/50 [00:14<00:31,  1.12it/s]Epoch: 16 | train_loss: 0.7818 | train_acc: 0.6953 | test_loss: 1.0603 | test_acc: 0.3826\n",
            " 32% 16/50 [00:15<00:30,  1.13it/s]Epoch: 17 | train_loss: 0.7852 | train_acc: 0.7148 | test_loss: 1.0120 | test_acc: 0.4328\n",
            " 34% 17/50 [00:16<00:29,  1.14it/s]Epoch: 18 | train_loss: 0.8705 | train_acc: 0.5586 | test_loss: 1.0652 | test_acc: 0.3816\n",
            " 36% 18/50 [00:17<00:28,  1.14it/s]Epoch: 19 | train_loss: 0.8630 | train_acc: 0.5312 | test_loss: 0.9399 | test_acc: 0.5634\n",
            " 38% 19/50 [00:18<00:27,  1.13it/s]Epoch: 20 | train_loss: 0.8435 | train_acc: 0.6250 | test_loss: 0.9662 | test_acc: 0.4839\n",
            " 40% 20/50 [00:19<00:28,  1.04it/s]Epoch: 21 | train_loss: 0.8432 | train_acc: 0.6016 | test_loss: 1.0891 | test_acc: 0.4129\n",
            " 42% 21/50 [00:20<00:32,  1.13s/it]Epoch: 22 | train_loss: 0.9394 | train_acc: 0.5078 | test_loss: 1.1670 | test_acc: 0.2917\n",
            " 44% 22/50 [00:21<00:30,  1.10s/it]Epoch: 23 | train_loss: 0.7828 | train_acc: 0.7148 | test_loss: 0.9688 | test_acc: 0.4536\n",
            " 46% 23/50 [00:22<00:27,  1.04s/it]Epoch: 24 | train_loss: 0.7356 | train_acc: 0.7305 | test_loss: 0.9277 | test_acc: 0.5331\n",
            " 48% 24/50 [00:23<00:25,  1.01it/s]Epoch: 25 | train_loss: 0.8308 | train_acc: 0.5742 | test_loss: 0.9696 | test_acc: 0.5350\n",
            " 50% 25/50 [00:24<00:24,  1.04it/s]Epoch: 26 | train_loss: 0.7488 | train_acc: 0.7500 | test_loss: 1.1268 | test_acc: 0.4025\n",
            " 52% 26/50 [00:25<00:22,  1.07it/s]Epoch: 27 | train_loss: 0.6318 | train_acc: 0.7383 | test_loss: 1.0509 | test_acc: 0.5152\n",
            " 54% 27/50 [00:26<00:21,  1.08it/s]Epoch: 28 | train_loss: 0.7903 | train_acc: 0.6289 | test_loss: 1.1567 | test_acc: 0.3428\n",
            " 56% 28/50 [00:27<00:20,  1.08it/s]Epoch: 29 | train_loss: 0.6544 | train_acc: 0.7070 | test_loss: 1.2449 | test_acc: 0.3116\n",
            " 58% 29/50 [00:28<00:19,  1.08it/s]Epoch: 30 | train_loss: 0.7064 | train_acc: 0.7773 | test_loss: 1.0823 | test_acc: 0.3532\n",
            " 60% 30/50 [00:28<00:18,  1.09it/s]Epoch: 31 | train_loss: 0.6690 | train_acc: 0.7461 | test_loss: 1.0508 | test_acc: 0.4451\n",
            " 62% 31/50 [00:29<00:17,  1.09it/s]Epoch: 32 | train_loss: 0.6010 | train_acc: 0.7734 | test_loss: 1.1286 | test_acc: 0.4148\n",
            " 64% 32/50 [00:30<00:16,  1.10it/s]Epoch: 33 | train_loss: 0.5625 | train_acc: 0.7656 | test_loss: 1.1683 | test_acc: 0.4347\n",
            " 66% 33/50 [00:32<00:17,  1.03s/it]Epoch: 34 | train_loss: 0.5320 | train_acc: 0.7812 | test_loss: 1.1175 | test_acc: 0.4441\n",
            " 68% 34/50 [00:33<00:18,  1.16s/it]Epoch: 35 | train_loss: 0.5293 | train_acc: 0.7969 | test_loss: 1.1918 | test_acc: 0.4650\n",
            " 70% 35/50 [00:34<00:16,  1.08s/it]Epoch: 36 | train_loss: 0.6102 | train_acc: 0.8008 | test_loss: 1.1894 | test_acc: 0.4138\n",
            " 72% 36/50 [00:35<00:14,  1.02s/it]Epoch: 37 | train_loss: 0.5753 | train_acc: 0.7617 | test_loss: 1.3668 | test_acc: 0.4157\n",
            " 74% 37/50 [00:36<00:12,  1.02it/s]Epoch: 38 | train_loss: 0.7137 | train_acc: 0.6328 | test_loss: 1.0066 | test_acc: 0.4839\n",
            " 76% 38/50 [00:37<00:11,  1.00it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/pytorch-learning/05_scripts_modularity/scripts/train.py\", line 66, in <module>\n",
            "    engine.train(model=model,\n",
            "  File \"/content/pytorch-learning/05_scripts_modularity/scripts/engine.py\", line 141, in train\n",
            "    test_loss, test_acc = test_step(model=model,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/pytorch-learning/05_scripts_modularity/scripts/engine.py\", line 87, in test_step\n",
            "    for batch, (X, y) in enumerate(dataloader):\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 732, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1470, in _next_data\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1618, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "        print(\"Downloading pizza, steak, sushi data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping pizza, steak, sushi data...\")\n",
        "        zip_ref.extractall(image_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUCleVuk5VAF",
        "outputId": "35fe3dbd-f5c6-41e1-b171-b88eb3e70648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -R"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgWriSLC5_5B",
        "outputId": "318a07dd-0f78-42b7-d111-efa617220648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".:\n",
            "\u001b[0m\u001b[01;34mscripts\u001b[0m/\n",
            "\n",
            "./scripts:\n",
            "data_set_load.py  engine.py  model_builder.py  \u001b[01;34m__pycache__\u001b[0m/  train.py  utils.py\n",
            "\n",
            "./scripts/__pycache__:\n",
            "data_set_load.cpython-312.pyc  model_builder.cpython-312.pyc\n",
            "engine.cpython-312.pyc         utils.cpython-312.pyc\n"
          ]
        }
      ]
    }
  ]
}